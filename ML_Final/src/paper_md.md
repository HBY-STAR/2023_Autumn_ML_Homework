# 选题
建立一个 Sales Volume Forecast 销量预测模型，根据过往数据预测给定某商品未来的销量情况，帮助企业更好地进行市场分析，库存管理，供应链优化等。

# 工作流程

## 一、问题分析

### （1）背景分析

在当今竞争激烈的市场环境中，企业为了更合理高效地运营和取得竞争优势，迫切需要准确的销量预测模型。在近几年的疫情中这一点显得尤为明显，若能准确预测产品的销量情况，无疑能大大提高企业的风险抵抗能力。然而随着市场环境的不断变化和销售环境的日益复杂，传统的统计方法已经不能满足需求，尤其是当要投放新产品时，传统的统计方法难以根据投放前的特征对其销量进行准确预测，使得企业在投放新产品时风险很高。因此，为了满足市场需求、优化库存管理、提高供应链效率、提高风险抵抗能力等，使用先进的机器学习方法建立一个强大的销量预测模型至关重要。

### （2）项目介绍

本项目根据某超市的历史订单数据，建立一个 Sales Volume Forecast 销量预测模型，在给定产品数据的情况下预测该产品未来四年（历史数据跨度为四年）在该超市的销量，从而帮助超市更好地进行市场分析，库存管理，供应链优化等。

## 二、数据预处理

### （1）初步特征选择

原始数据中存在很多对模型训练无关紧要的特征，因此首先要对数据进行过滤，分析数据中的哪些特征属于无关特征并除去。
模型的输入为产品信息，输出为销量的预测值。
原始数据的特征为：订单Id，利润率，记录数，制造商，产品名称，利润，发货日期，国家，地区，城市，子类别，客户名称，折扣，数量，省/自治区，类别，细分，订单日期，邮寄方式，销售额。
在原始数据中，订单Id，发货日期，客户名称，细分，城市，省份，地区，订单日期这些特征与模型的输入输出无关，为防止无关数据导致模型过拟合，应去除；记录数和国家在所有数据中都相同，对模型的训练没有帮助，也应去除。注意到邮寄方式作为产品的副加服务，对产品销量也应当有一定的影响应当保留。
使用以下代码剔除无关特征并将新数据保存到processed.xlsx中。
```python

```
processed.xlsx的数据如下所示：

```
```

### （2）处理数据缺失

注意到上面的数据中存在空白值，使用以下代码找到数据中的空白的列
```python
```
输出为：
```sh
```
故只在 '利润' 这一列存在空白数据，注意到利润即为销售额与利润率的乘积，而销售额和利润率均不含空数据，故利润的全部空值都可以通过销售额乘利润率进行填充，注意到销售额和利润均含有￥符号，故去除￥使其转化为数值类型再进行填充。代码如下：
```python

```
processed.xlsx之后的数据如下所示，利润空值均被填充了。

```
```

由于其他列没有检测到空值，故该表的空值填充完成。

### （3）特征拆分

注意到产品名称这一特征包含了制造商，产品名和颜色三个特征，为提高模型的可解释性，同时使机器学习算法更好地理解数据并发掘潜在信息，从而提高模型性能，将其拆分为三个特征。其中制造商特征与已存在的特征重合，故直接去掉。采用下面的代码进行特征拆分。
```python

```

### （4）特征转换

首先，注意到：销售额 = 单个售价 * 折扣 * 数量； 利润 = 销售额 * 利润率。而数量是模型需要进行预测的值，销售额、利润和数量直接关联，故若直接采用销售额和利润这两个特征进行训练，则难以探索这两个特征与数量直接的关系。因应剔除其中内含的与数量的直接关联。令 单个售价 = 销售额 / 数量； 单个利润 = 利润 / 数量，并用单个售价替代销售额，用单个利润替代利润。这样模型可以更多地探索单个售价、单个利润和销量的潜在关系，从而减少过拟合。
使用如下代码进行上述过程：
```python

```
processed.xlsx之后的数据如下所示，销售额和利润特征被转换了。

```
```

另外，由于模型要预测的是某产品的销量，而目前数据中的特征为每个订单的数量，故需要将数量特征转换为销量特征。
运行以下代码：
```python

```
processed.xlsx之后的数据如下所示，将数量转换为新的销量特征。

```
```

### （5）处理噪点
考虑到数据集中可能存在不合理的数据，因此绘制非类型变量的箱型图来进行观察：
```python

```
结果如下：

由上图可以看出在折扣这一特征中出现了0.8这个非常突兀的值，根据日常生活经验可知这种程度的折扣应属于特殊情况，在正常情况下折扣力度不应如此大，这种折扣也是导致利润率出现较多离群值的原因。通过以下代码检测出0.8折的比例。
```python

```
输出如下：
可以看出0.8折所占比例很小，仅有1.6%，可以认为是噪点并去除
```python

```
之后绘制箱型图如下：

虽然仍存在离群值，但总体上来看比之前更为合理，且0.4折是导致离群值的主要原因，其数据占总数据量的20.85%，比例较大，不可认为是噪点。

### （6）数值化：
现在得到的数据中还有很多类型变量，需要将其转换为数值类型，运行以下代码获取每个特征中类型的数量：
```python

```
获取输出如下：
```sh

```
可以看出有的特征类型数量较多，而有的较少。考虑到独热编码对于种类较多的类型会引发维度爆炸，而目标编码可能会丢失一些数据原本的信息，故采用独热编码和目标编码混合的方式进行数值化，对于类别较少的类型变量尽量采用独热编码，而对于类别较多的。对地区，类别和邮寄方式采用独热编码，对其他特征采用目标编码。
```python

```
处理后的数据如下：



### （7）进一步特征选择

首先通过绘制热图观察目前数据中各特征之间的相关性



由于绝对值小于0.05的相关性被认为是非常弱的，故为了降低特征维度，提高模型的泛化能力，可尝试去掉绝对值小于0.05的特征来进行训练。但由于热图计算的是线性相关性，而各种特征之间的关系可能不止线性，故不可贸然地直接去掉，而应同时训练这两种数据，比较并选择更优的模型。使用以下代码创建新的数据表并分别命名为 feature_all.xlsx 和 feature_selected.xlsx。分别对应未去除相关性小于0.05的特征的数据和去除去除相关性小于0.05的特征的数据。这两个文件即为接下来要进行模型训练的数据。
```python

```


## 三、模型训练测试

### （1）选定测试标准

回归分析的评价指标有很多，常见的有均方误差，均方根误差，平均绝对误差，确定系数等。这里选择均方误差MSE和确定系数R^2作为评价指标。MSE 是预测误差的平方和的平均值，这种平方操作可以消除正负误差的影响，同时有利于数学计算和优化。MSE 对于异常值（离群点）具有较强的敏感性，这意味着它会更强烈地惩罚那些预测误差较大的数据点，使得模型更关注预测值与实际值的接近程度。R-squared 衡量了模型对因变量变化的解释程度，其值在0到1之间，越接近1表示模型能够更好地解释目标变量的变化。这使得 R-squared 对于向非专业人员解释模型性能更加直观。R-squared 提供了一种相对比较不同模型拟合能力的方法。在比较不同模型时，可以通过 R-squared 值的大小来判断哪个模型更好地拟合了数据。
实际进行模型训练时，将经过上面预处理的数据按照8:2分为训练集和测试集。在使用训练集训练模型时，采用网格搜索的方式寻找最优超参数，网格搜索采用负均方误差作为搜索指标。模型训练完成后，使用测试集来检验模型的性能，采用MSE和R^2结合的方式进行评价，并选出最佳的模型。

### （2）岭回归模型
岭回归（Ridge Regression）是一种线性回归的改进方法，它通过在损失函数中添加一个正则化项，可以有效地应对多重共线性的问题。

本项目使用的岭回归API为：sklearn.linear_model.Ridge。网格搜索API为：sklearn.model_selection.GridSearchCV。通过如下代码引入这些库：

```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
```
此外，为了加速模型的训练，使用了 intel 的 sklearn 扩展。
之后的几个模型也尽量使用该扩展来加速模型的训练。要启动该加速扩展，
只需在导入需要的sklearn API 之前引入以下代码：
```python
from sklearnex import patch_sklearn
patch_sklearn()
```

Ridge的主要超参数如下：

（1）alpha：控制正则化的强度。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5, 6, 7, 8 , 9, 10, 11, 12, 13, 14, 15.0, 17.5, 20.0]
}
```

上面的范围是经过多次测试得到的。一开始为了逐步探索，搜索范围可以比较小，然后训练模型并观察搜索出的最佳超参数。如果是在搜索范围某个项的边界处，则适当扩大该项在此边界处的搜索范围。如果是在搜索范围某个项的中间某个值，则在该值的周围细化搜索范围。然后继续训练模型并观察搜索出的最佳超参数，直到该超参数在搜索范围中比较符合预期。

模型训练的代码如下：
```python
# 定义超参数的搜索范围
param_grid = {
    'alpha': [0.1, 0.5, 1.0, 2.0, 5, 6, 7, 8 , 9, 10, 11, 12, 13, 14, 15.0, 17.5, 20.0]
}
# 读取数据
df_all = pd.read_excel('feature_all.xlsx')
X_all = df_all.drop(['销量'],axis=1)
y_all = df_all['销量']
X_all, y_all = np.array(X_all), np.array(y_all)
# 划分数据集为训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)
# 创建岭回归模型
model = Ridge()
# 创建网格搜索对象
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')
# 在训练集上进行网格搜索
grid_search.fit(X_train, y_train)
# 输出最佳超参数和对应的模型性能
print("最佳超参数:", grid_search.best_params_)
print("最佳模型性能 (负均方误差):", grid_search.best_score_)
# 在测试集上评估模型
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print("在测试集上的均方误差:", mse)
r2 = r2_score(y_test, y_pred)
print("在测试集上的R²值:", r2)
```

代码的主要思路为：首先定义超参数的搜索范围，然后读取之前预处理好的数据并转化为numpy array以便于模型的使用。之后划分数据集为训练集和测试集，按照8:2的比例划分。之后使用上面所述的库构建模型并执行网格搜索。由于网格搜索通过最大化评价指标的值来选择最佳超参数，而均方误差是一个损失函数，我们希望最小化它，因此在网格搜索中使用负均方误差，使其成为一个需要最大化的指标。网格搜索采用五折交叉验证，这与之前的训练集与测试集8:2相对应。之后在测试集上评估模型，计算并输出在测试集上的均方误差MSE和确定系数R^2。

由于之后几个模型代码的主要思路均与此类似，基本上只有模型API和超参数搜索范围的差别，故在接下来的模型中不再详细介绍代码实现，而是关注超参数搜索范围和模型输出结果。

岭回归模型的输出结果如下：
```sh
===================ALL====================
最佳超参数: {'alpha': 10}
最佳模型性能 (负均方误差): -5.332307979513909
在测试集上的均方误差: 5.9785274330344835
在测试集上的R²值: 0.171708125740204
===================ALL====================

================SELECTED==================
最佳超参数: {'alpha': 8}
最佳模型性能 (负均方误差): -5.331018094722255
在测试集上的均方误差: 5.987458697258844
在测试集上的R²值: 0.17047074853205924
================SELECTED==================
```
上面的all指的是未去掉相关性小于0.05的数据，selected指的是去掉相关性小于0.05的数据结果，在数据预处理中步骤（7）中有具体描述，关于此项之后也不再赘述。

### （3）Elastic Net回归模型
Elastic Net回归是一种结合了Lasso回归（L1正则化）和Ridge回归（L2正则化）的线性回归方法。它综合了Lasso回归的稀疏性和Ridge回归的正则化特性，通过两种正则化项来控制模型的复杂度。

本项目使用的Elastic Net回归API为：sklearn.linear_model.ElasticNet。

ElasticNet的主要超参数如下：

（1）alpha：Elastic Net正则化惩罚项的权重，结合了L1和L2正则化项。

（2）l1_ratio：L1和L2正则化项的混合比例。

（3）max_iter：迭代的最大次数。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'alpha': [0.001, 0.002, 0.003, 0.004, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0],
    'l1_ratio': [0.001, 0.002, 0.003, 0.004, 0.005, 0.01, 0.05, 0.1, 0.5], 
    'max_iter': [500, 1000, 1500],
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'alpha': 0.002, 'l1_ratio': 0, 'max_iter': 500}
最佳模型性能 (负均方误差): -5.332313601393271
在测试集上的均方误差: 5.978555438754639
在测试集上的R²值: 0.171704245702737
===================ALL====================

================SELECTED==================
最佳超参数: {'alpha': 0.002, 'l1_ratio': 0, 'max_iter': 500}
最佳模型性能 (负均方误差): -5.331042748471995
在测试集上的均方误差: 5.987393273302495
在测试集上的R²值: 0.1704798126589756
================SELECTED==================
```

### （4）决策树回归模型
决策树回归是一种非常灵活的回归算法，它通过对特征空间进行递归划分来建立模型。在每个节点上，决策树根据某个特征的阈值将数据集划分为两个子集，然后在子集上递归地重复这个过程，直到达到停止条件。在预测时，样本通过树的分支最终到达叶子节点，叶子节点的值即为预测值。决策树算法易于理解，构建较快且容错能力较强。但决策树模型非常容易过拟合，导致泛化能力不强。

本项目使用的决策树回归API为：sklearn.tree.DecisionTreeRegressor。

DecisionTreeRegressor的主要超参数如下：

（1）max_depth：决策树的最大深度。

（2）min_samples_split：节点分裂的最小样本数。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'max_depth': [3, 4, 5, 10, 15, 20, 25, 30],
    'min_samples_split': [2, 5, 10, 15, 20, 25, 30],
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'max_depth': 5, 'min_samples_split': 30}
最佳模型性能 (负均方误差): -5.079826147372448
在测试集上的均方误差: 5.790111438766902
在测试集上的R²值: 0.19781211853449743
===================ALL====================

================SELECTED==================
最佳超参数: {'max_depth': 5, 'min_samples_split': 30}
最佳模型性能 (负均方误差): -5.079832531342071
在测试集上的均方误差: 5.792199494509841
在测试集上的R²值: 0.19752282997234738
================SELECTED==================
```

### （5）随机森林回归模型
随机森林回归是一种基于决策树的集成学习方法。它通过构建多个决策树，并将它们的预测结果进行平均来提高模型的性能。

本项目使用的随机森林回归API为：sklearn.ensemble.RandomForestRegressor

RandomForestRegressor的主要超参数如下：

（1）n_estimators：森林中树的数量。

（2）max_depth：每棵树的最大深度。

（3）min_samples_split：节点分裂的最小样本数。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'n_estimators': [100, 200, 300],  # 决策树的数量
    'max_depth': [5, 6, 7, 8, 9, 10, 15],  # 决策树的最大深度
    'min_samples_split': [10, 15, 20, 25, 30]
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'max_depth': 7, 'min_samples_split': 30, 'n_estimators': 300}
最佳模型性能 (负均方误差): -4.818352669840546
在测试集上的均方误差: 5.419199597793122
在测试集上的R²值: 0.2491999004568074
===================ALL====================

================SELECTED==================
最佳超参数: {'max_depth': 7, 'min_samples_split': 25, 'n_estimators': 300}
最佳模型性能 (负均方误差): -4.819712895045467
在测试集上的均方误差: 5.426281928393181
在测试集上的R²值: 0.24821868276523407
================SELECTED==================
```

### （6）XGBoost回归模型
XGBoost（eXtreme Gradient Boosting）是一种梯度提升树算法，被广泛用于回归和分类问题。XGBoost通过组合多个弱学习器来构建一个强大的模型。

本项目使用的XGBoost回归API为：xgboost.XGBRegressor

XGBRegressor的主要超参数如下：

（1）learning_rate：学习率，用于控制每棵树对最终模型的贡献。

（2）n_estimators：弱学习器的数量。

（3）max_depth：每棵树的最大深度。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'learning_rate': [0.01, 0.03, 0.05, 0.07, 0.1, 0.2],
    'n_estimators': [25, 50, 75, 100, 150, 200],
    'max_depth': [3, 4, 5, 6, 7, 10, 15],
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'learning_rate': 0.07, 'max_depth': 3, 'n_estimators': 150}
最佳模型性能 (负均方误差): -4.7580901491048575
在测试集上的均方误差: 5.335830892275311
在测试集上的R²值: 0.2607501730149635
===================ALL====================

================SELECTED==================
最佳超参数: {'learning_rate': 0.07, 'max_depth': 3, 'n_estimators': 150}
最佳模型性能 (负均方误差): -4.759016080615333
在测试集上的均方误差: 5.34364093607743
在测试集上的R²值: 0.25966813468840766
================SELECTED==================
```

### （7）梯度提升树回归模型
梯度提升树（Gradient Boosting Trees）是一种集成学习算法，利用加法模型和前向分步算法实现学习的优化过程。在回归问题中，该算法通过不断减小残差的梯度来逐步逼近目标值。

本项目使用的梯度提升树回归API为：sklearn.ensemble.GradientBoostingRegressor

GradientBoostingRegressor的主要超参数如下：

（1）n_estimators：弱分类器的数量。

（2）learning_rate：学习率，用于控制每棵树对最终模型的贡献。

（3）max_depth：每棵树的最大深度。

（4）min_samples_split：节点分裂所需的最小样本数。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [3, 4, 5, 6, 7, 10, 15, 20],
    'min_samples_split': [2, 5, 10, 20, 30, 40],
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 20, 'n_estimators': 200}
最佳模型性能 (负均方误差): -4.737767406988179
在测试集上的均方误差: 5.304333683406016
在测试集上的R²值: 0.26511393691176277
===================ALL====================

================SELECTED==================
最佳超参数: {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 30, 'n_estimators': 150}
最佳模型性能 (负均方误差): -4.737843122946775
在测试集上的均方误差: 5.3351530045104365
在测试集上的R²值: 0.2608440906114945
================SELECTED==================
```


### （8）LightGBM回归模型
LightGBM（Light Gradient Boosting Machine）是一种梯度提升框架，专注于高效性和分布式计算。它是一种基于树的学习算法，对于大规模数据集和高维特征的情况下表现出色。

本项目使用的LightGBM回归API为：lightgbm.LGBMRegressor

LGBMRegressor的主要超参数如下：

（1）n_estimators：树的数量。

（2）learning_rate：学习率，控制每棵树的影响程度。

（3）max_depth：每棵树的最大深度。

（4）min_child_samples：每个叶子节点所需的最小样本数。

在本项目中超参数的搜索范围如下：
```python
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
    'n_estimators': [50, 75, 100, 150],
    'max_depth': [3, 4, 5, 6, 7, 10, 15, 20, 25],
    'min_child_samples': [15, 20, 25, 30, 35, 40],
}
```

输出结果如下：
```sh
===================ALL====================
最佳超参数: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 30, 'n_estimators': 100}
最佳模型性能 (负均方误差): -4.750771687261698
在测试集上的均方误差: 5.362673731712184
在测试集上的R²值: 0.25703124623296525
===================ALL====================

================SELECTED==================
最佳超参数: {'learning_rate': 0.1, 'max_depth': 3, 'min_child_samples': 40, 'n_estimators': 100}
最佳模型性能 (负均方误差): -4.748888975163311
在测试集上的均方误差: 5.359032198540633
在测试集上的R²值: 0.25753576049163995
================SELECTED==================
```

### （5）分析并选定算法

对上面算法的结果进行统计并画图如下：

可以看出，无论是在MSE上还是R^2上，梯度提升树的性能都是最好的。在训练的过程中可以发现GBDT的训练速度是最慢的，而xgboost和lightbm作为梯度提升树在速度上优化的算法，训练速度明显快了很多，而性能也只是略差。故在数据量比较大和对时间比较敏感的情况下，可以采用xgboost和lightgbm来较快地得到结果。

## 五、总结
